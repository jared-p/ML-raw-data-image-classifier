{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "894ec832",
   "metadata": {},
   "source": [
    "# ENGR481 Project Stage 1\n",
    "\n",
    "By: Jared Paull (63586572), Liam Ross (75469692)\n",
    "\n",
    "\n",
    "The first step that must be taken is to import the necessary libraries for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e19804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ab744",
   "metadata": {},
   "source": [
    "## Scraping Image Data\n",
    "\n",
    "Next, the training data must be loaded into a 2D numpy array. The first dimention of the array represents a sample, which in this context is a picture. Then each pixel contained of an image represents a feature of the dataset. This means that the total number of features is equal to the total number of pixels in the image.\n",
    "\n",
    "To do this, first the training images must be loaded into an array. Each image must be grayscaled, normalized in size, then scaled down to reduce the memory requirements of the algorithm. Simultaniously, each sample must have a corresponding label so that the algorithm can later correctly train a model from the data paired with the correct labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b2de2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[131 131 132 ... 159 159 160]\n",
      " [208 207 208 ... 199 199 200]\n",
      " [206 206 206 ... 196 195 196]\n",
      " ...\n",
      " [143 143 142 ... 197 196 196]\n",
      " [180 181 181 ... 206 206 205]\n",
      " [196 196 195 ... 220 220 222]] \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# pass in the relative directory that contains the training data.\n",
    "# directory is relative to where this Jupyter notebook is\n",
    "# Refer to get_image_data function at the bottom for detailed comments on the function.\n",
    "x, y = get_image_data(\"../data/training\")\n",
    "\n",
    "# print statement to get a feel for the data\n",
    "# Each row of x are all 4096 pixels of an image\n",
    "# Each value of y indicates the class, where the index of y correlates to the row of x (linking image data and label).\n",
    "print(x,\"\\n\", y);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17ac24e",
   "metadata": {},
   "source": [
    "### Creating Logistic Regression Model\n",
    "\n",
    "Now that all of the image data is collected, and they have a corresponding label. The data can be fit to a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06caf76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating logistic regression model instance that implements a liblinear solver type\n",
    "# liblinear solver implements a coordinate descent algorithm which works well with high dimension (4096 here)\n",
    "log_regress = linear_model.LogisticRegression(solver = \"liblinear\")\n",
    "# method to fit the logistic regression instance with the data collected in the previous cell\n",
    "log_regress.fit(x,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c0173a",
   "metadata": {},
   "source": [
    "## Testing the algorithm\n",
    "\n",
    "Now that a model exists, image data and labels are scraped from the testing folder, in the exact same fashion as the data collection from the training data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdee8d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code here is the same as that used to get image data from the training folder.\n",
    "# This section will not be commented on, since the previous section covers all aspects of it.\n",
    "# xt,yt represent image (xt) training data, and label (yt) training data\n",
    "\n",
    "xt, yt = get_image_data(\"../data/testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af04cdda",
   "metadata": {},
   "source": [
    "## Prediction and Confusion Matrix\n",
    "\n",
    "The training data is fed into the model and an output is predicted (based on the model). Then the outputs from the model are compared with the correct values to see the model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8eccac18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape      Circle  Rectangle  Square\n",
      "Circle         17          0       1\n",
      "Rectangle       0         17       1\n",
      "Square          1          0      17\n",
      "\n",
      "Percentage of model errors from the training data: 5.56%\n"
     ]
    }
   ],
   "source": [
    "# feed the training data into the model, pred is an array containing the output labels based on the model\n",
    "pred =  log_regress.predict(xt)\n",
    "\n",
    "# These are two formatting questions to make the confusion matrix more appealing. Refer to confusion_format function at the bottom.\n",
    "predicted = confusion_format(pred)\n",
    "actual = confusion_format(y)\n",
    "\n",
    "# prints a confusion matrix, rows are true values, and columns are the model's guessed values.\n",
    "print(pd.crosstab(actual, predicted, rownames=[None], colnames=[\"Shape\"]))\n",
    "\n",
    "# then the percentage of errors is the number of errors divided by the total number of image samples times 100 for percentage.\n",
    "# The error_percentage function is described below in comment detail.\n",
    "print(f\"\\nPercentage of model errors from the training data: {error_percentage(pred,y):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b38863",
   "metadata": {},
   "source": [
    "## Model Accuracy\n",
    "\n",
    "Baised on the testing data, the trained model is ~94.5% accurate.\n",
    "\n",
    "## Weight Visualization\n",
    "\n",
    "Now that a model has been trained, and has been validated as having over 94% correctness the weights can be visualized. A plot can be formed of each weighting vector. The function below will call a slew of functions to stitch together three visualizations, from left to right the images are as follows: circle weights, rectangle weights, square weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7db6fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling factor for weights so visible on monochrome scale\n",
    "# ~250-500 works well\n",
    "weight_scaling_factor = 500\n",
    "show_weights(log_regress, weight_scaling_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff68ced",
   "metadata": {},
   "source": [
    "## Functions Used Elsewhere\n",
    "\n",
    "The first function is used to get all image data from a relative directory, it is used to get all image data for training and testing. Thus images are read from both of their respective directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7ec0b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_data(rel_dir):\n",
    "\n",
    "    # first, an empty list is created so that image pixel arrays can be later added to it\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "\n",
    "    # create a for loop that will iterate through all items in the relative directory that contains the image data\n",
    "    for pic in os.listdir(f\"{rel_dir}/\"):\n",
    "        # import image using Pillow library, then convert the image to grayscale imediatly\n",
    "        image = PIL.Image.open(f\"{rel_dir}/{pic}\").convert(\"L\")\n",
    "        # crop the image, will crop vertically from height/4 to 3*height/4\n",
    "        # and crop horizontally from width/4 to 3*height/4\n",
    "        # this will crop the image to reduce memory to only relevant pixels\n",
    "        image = PIL.Image.fromarray(np.array(image)[int(np.floor(image.height / 4)) : int(np.ceil(3 * image.height / 4)), int(np.floor(image.width / 4)) : int(np.ceil( 3 * image.width / 4))])\n",
    "        # resizes the image to 64x64 pixels, ensures the number of feature vectors are constant regardless of raw image file.\n",
    "        # resizing also reduces the total memory requirements of the algorithm.\n",
    "        image = image.resize((64,64))\n",
    "        # converts from image format to a 2D array representing a pixel grid\n",
    "        data = np.asarray(image)\n",
    "        # converts from a 2D pixel grid to a 1D array of length 64^2=4096, where the rows are appended horizontally.\n",
    "        vec = np.hstack(data)\n",
    "        # add the image data to the container of all images.\n",
    "        x.append(vec)\n",
    "    \n",
    "        # examine the name of the picture file, can find correct label based on first letter of the file name.\n",
    "        # c indicates the picture is a circle\n",
    "        if( str.lower(pic[0]) == \"c\"):\n",
    "            # classify circles as a 0\n",
    "            y.append(0)\n",
    "        # r indicates the picture is a rectangle\n",
    "        elif (str.lower(pic[0]) == \"r\"):\n",
    "            # classify rectangle as a 1\n",
    "            y.append(1)\n",
    "        # only other situation is the image is a square\n",
    "        else:\n",
    "            # classify square as a 2\n",
    "            y.append(2)\n",
    "    \n",
    "    # convert from python list to numpy array, format is required for sklearn logistic regression solver.\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7694ae",
   "metadata": {},
   "source": [
    "The next function is used to format the confusion matrix, instead of having rows correlating to the decimal value. This function converts from decimal values to corresponding string depending on the class map that was definited initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b4a1d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will convert from decimal label to strings.\n",
    "# 0=>Circle, 1=>Rectangle, 2=>Square\n",
    "\n",
    "def confusion_format(labels):\n",
    "    test = []\n",
    "    for i in labels:\n",
    "        if i == 0:\n",
    "            test.append(\"Circle\")\n",
    "        elif i == 1:\n",
    "            test.append(\"Rectangle\")\n",
    "        else:\n",
    "            test.append(\"Square\")\n",
    "    test = np.array(test)\n",
    "    return test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a249b9f",
   "metadata": {},
   "source": [
    "The final function is used to find the percentage difference between two arrays. This is used to find the error of the model's classification compared to the proper classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d4f0963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_percentage(pred, y):\n",
    "    \n",
    "    # the number of errors is the number of differences between the model's labels and the correct labels\n",
    "    errors = 0\n",
    "    for i in range(pred.size):\n",
    "        # pred is the predicted array labels, while y is the actual\n",
    "        if pred[i] != y[i]:\n",
    "            errors = errors + 1\n",
    "            \n",
    "    # then the percentage of errors is the number of errors divided by the total number of image samples times 100 for percentage.\n",
    "    return errors / pred.size * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ea063d",
   "metadata": {},
   "source": [
    "Moving onto the visualization section. This function will call many functions to orchestrate the view. These functions will be discussed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5e3c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_weights(log_regress, scale):\n",
    "    c0 = log_regress.coef_[0]\n",
    "    c1 = log_regress.coef_[1]\n",
    "    c2 = log_regress.coef_[2]\n",
    "    \n",
    "    i0 = get_weight_image(c0, scale)\n",
    "    i1 = get_weight_image(c1, scale)\n",
    "    i2 = get_weight_image(c2, scale)\n",
    "    \n",
    "    i = concate_horizontal_images(i0,i1,i2)\n",
    "    i.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b963d8",
   "metadata": {},
   "source": [
    "The function below will convert from a weighting vector into an image. It will take the weighting vectors, then normalize by the max value weight, then scale it by a factor to cover the monochrome colour range. Afterwards it must convert from a single array of length 4096 to a 2D array where both dimensions have length 64. Finally, the array is converted back into an image and it is resized to be easily visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fe99c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_image(c, scale):\n",
    "    # coefficients normalized and scaled by a factor\n",
    "    c = np.abs(c/c.max()) * scale\n",
    "    \n",
    "    # initialize empty list to store rows\n",
    "    a = []\n",
    "    for i in range(64):\n",
    "        temp = []\n",
    "        for j in range(64):\n",
    "            temp.append(c[64 * i + j])\n",
    "        a.append(temp)\n",
    "    a = np.array(a)\n",
    "    \n",
    "    # convert from array to image\n",
    "    i = PIL.Image.fromarray(a)\n",
    "    # return the resized image\n",
    "    return i.resize((256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108140f6",
   "metadata": {},
   "source": [
    "The last function for visualization will stitch together each of the three weighting images into one single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d17c934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concate_horizontal_images(im0, im1, im2):\n",
    "    i = PIL.Image.new(mode = \"L\",size=(768, 256))\n",
    "    i.paste(im0, (0, 0, 256, 256))\n",
    "    i.paste(im1, (256, 0))\n",
    "    i.paste(im2, (512, 0))\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dd978d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
